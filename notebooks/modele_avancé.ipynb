{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "058049da-3988-45b8-b89f-fef63fe8483f",
   "metadata": {},
   "source": [
    "# 1. Importation des Bibliothèques et Chargement des Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cfdfa395-7c3c-4f50-a461-36d2dea399b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fasttext\n",
      "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
      "     ---------------------------------------- 0.0/73.4 kB ? eta -:--:--\n",
      "     --------------- --------------------- 30.7/73.4 kB 660.6 kB/s eta 0:00:01\n",
      "     ------------------------------------- 73.4/73.4 kB 810.2 kB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting pybind11>=2.2 (from fasttext)\n",
      "  Using cached pybind11-2.13.5-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in d:\\master_openclassroom\\python\\lib\\site-packages (from fasttext) (68.2.2)\n",
      "Requirement already satisfied: numpy in d:\\master_openclassroom\\python\\lib\\site-packages (from fasttext) (1.26.4)\n",
      "Using cached pybind11-2.13.5-py3-none-any.whl (240 kB)\n",
      "Building wheels for collected packages: fasttext\n",
      "  Building wheel for fasttext (pyproject.toml): started\n",
      "  Building wheel for fasttext (pyproject.toml): finished with status 'error'\n",
      "Failed to build fasttext\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for fasttext (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [33 lines of output]\n",
      "  C:\\Users\\trist\\AppData\\Local\\Temp\\pip-build-env-h7g7gosw\\overlay\\Lib\\site-packages\\setuptools\\dist.py:452: SetuptoolsDeprecationWarning: Invalid dash-separated options\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          Usage of dash-separated 'description-file' will not be supported in future\n",
      "          versions. Please use the underscore name 'description_file' instead.\n",
      "  \n",
      "          By 2024-Sep-26, you need to update your project and remove deprecated calls\n",
      "          or your builds will no longer be supported.\n",
      "  \n",
      "          See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    opt = self.warn_dash_deprecation(opt, section)\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-311\n",
      "  creating build\\lib.win-amd64-cpython-311\\fasttext\n",
      "  copying python\\fasttext_module\\fasttext\\FastText.py -> build\\lib.win-amd64-cpython-311\\fasttext\n",
      "  copying python\\fasttext_module\\fasttext\\__init__.py -> build\\lib.win-amd64-cpython-311\\fasttext\n",
      "  creating build\\lib.win-amd64-cpython-311\\fasttext\\util\n",
      "  copying python\\fasttext_module\\fasttext\\util\\util.py -> build\\lib.win-amd64-cpython-311\\fasttext\\util\n",
      "  copying python\\fasttext_module\\fasttext\\util\\__init__.py -> build\\lib.win-amd64-cpython-311\\fasttext\\util\n",
      "  creating build\\lib.win-amd64-cpython-311\\fasttext\\tests\n",
      "  copying python\\fasttext_module\\fasttext\\tests\\test_configurations.py -> build\\lib.win-amd64-cpython-311\\fasttext\\tests\n",
      "  copying python\\fasttext_module\\fasttext\\tests\\test_script.py -> build\\lib.win-amd64-cpython-311\\fasttext\\tests\n",
      "  copying python\\fasttext_module\\fasttext\\tests\\__init__.py -> build\\lib.win-amd64-cpython-311\\fasttext\\tests\n",
      "  running build_ext\n",
      "  building 'fasttext_pybind' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for fasttext\n",
      "ERROR: Could not build wheels for fasttext, which is required to install pyproject.toml-based projects\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fasttext'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, classification_report\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfasttext\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfasttext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fasttext'"
     ]
    }
   ],
   "source": [
    "!pip install fasttext\n",
    "\n",
    "# Importation des bibliothèques nécessaires\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Conv1D, MaxPooling1D, Flatten, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "import nltk\n",
    "\n",
    "# Assurez-vous que les ressources NLTK nécessaires sont téléchargées\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Charger les données déjà prétraitées\n",
    "data = pd.read_csv('../data/database_p7_rework.csv')\n",
    "\n",
    "# Paramètres de configuration\n",
    "MAX_SEQUENCE_LENGTH = 100  # Longueur maximale des séquences\n",
    "EMBEDDING_DIM = 300  # Dimension des vecteurs d'embedding FastText\n",
    "VOCAB_SIZE = 10000  # Taille maximale du vocabulaire\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35114828-42bd-4855-9fbc-4246df095d0f",
   "metadata": {},
   "source": [
    "# 2. Préparation des séquence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55edde65-9f8c-405b-91a9-51b3894a4a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choisir le texte lemmatisé ou stemmé\n",
    "text_column_lemma = 'text_lemmatized'\n",
    "text_column_stem = 'text_stemmed'\n",
    "\n",
    "# Tokenization des textes lemmatisés\n",
    "tokenizer_lemma = Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer_lemma.fit_on_texts(data[text_column_lemma])\n",
    "sequences_lemma = tokenizer_lemma.texts_to_sequences(data[text_column_lemma])\n",
    "\n",
    "# Tokenization des textes stemmés\n",
    "tokenizer_stem = Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer_stem.fit_on_texts(data[text_column_stem])\n",
    "sequences_stem = tokenizer_stem.texts_to_sequences(data[text_column_stem])\n",
    "\n",
    "# Padding des séquences pour obtenir des longueurs uniformes\n",
    "X_lemma = pad_sequences(sequences_lemma, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_stem = pad_sequences(sequences_stem, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# Label des cibles\n",
    "y = data['target']\n",
    "\n",
    "# Séparation des données en ensemble d'entraînement et de test pour les deux méthodes\n",
    "X_train_lemma, X_test_lemma, y_train_lemma, y_test_lemma = train_test_split(X_lemma, y, test_size=0.2, random_state=42)\n",
    "X_train_stem, X_test_stem, y_train_stem, y_test_stem = train_test_split(X_stem, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96493562-e2a8-4b9a-8071-df3d1834a198",
   "metadata": {},
   "source": [
    "# 3. Préparation des Embeddings Textuels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852b6574-2e5f-4f5b-8089-19aff592e2b3",
   "metadata": {},
   "source": [
    "## 3.1 Embeddings avec Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0920aae4-fe76-42c4-aa1f-ae63d193e4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraîner le modèle Word2Vec sur les textes lemmatisés\n",
    "w2v_model_lemma = Word2Vec(sentences=[text.split() for text in data[text_column_lemma]], vector_size=EMBEDDING_DIM, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Entraîner le modèle Word2Vec sur les textes stemmés\n",
    "w2v_model_stem = Word2Vec(sentences=[text.split() for text in data[text_column_stem]], vector_size=EMBEDDING_DIM, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Extraire les embeddings pour le vocabulaire lemmatisé (300 Features)\n",
    "embedding_matrix_word2vec_lemma = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "for word, i in tokenizer_lemma.word_index.items():\n",
    "    if i < VOCAB_SIZE:\n",
    "        try:\n",
    "            embedding_vector = w2v_model_lemma.wv[word]\n",
    "            embedding_matrix_word2vec_lemma[i] = embedding_vector\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "# Extraire les embeddings pour le vocabulaire stemmé (300 Features)\n",
    "embedding_matrix_word2vec_stem = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "for word, i in tokenizer_stem.word_index.items():\n",
    "    if i < VOCAB_SIZE:\n",
    "        try:\n",
    "            embedding_vector = w2v_model_stem.wv[word]\n",
    "            embedding_matrix_word2vec_stem[i] = embedding_vector\n",
    "        except KeyError:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258ba4d3-fe04-465d-ab07-97262f4f8dab",
   "metadata": {},
   "source": [
    "## 3.2  Embeddings avec FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9706ae25-3432-421f-b1ad-ff8341cec572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Téléchargement et chargement des vecteurs FastText (par exemple pour l'anglais)\n",
    "fasttext.util.download_model('en', if_exists='ignore')  # Télécharge les vecteurs FastText\n",
    "ft = fasttext.load_model('cc.en.300.bin')  # Charge le modèle (300 dimensions par défaut)\n",
    "\n",
    "# Embedding matrix pour lemmatisation (300 Features)\n",
    "embedding_matrix_fasttext_lemma = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "for word, i in tokenizer_lemma.word_index.items():\n",
    "    if i < VOCAB_SIZE:\n",
    "        embedding_matrix_fasttext_lemma[i] = ft.get_word_vector(word)\n",
    "\n",
    "# Embedding matrix pour stemming (300 Features)\n",
    "embedding_matrix_fasttext_stem = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "for word, i in tokenizer_stem.word_index.items():\n",
    "    if i < VOCAB_SIZE:\n",
    "        embedding_matrix_fasttext_stem[i] = ft.get_word_vector(word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5498ceb-37cf-40b5-99b1-05b46026dead",
   "metadata": {},
   "source": [
    "## 3.3 Concaténation de FastText et Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2b28bd-16b6-460d-8553-44673130863e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concaténation des embeddings FastText et Word2Vec pour lemmatisation (600 Features)\n",
    "embedding_matrix_concat_lemma = np.concatenate((embedding_matrix_fasttext_lemma, embedding_matrix_word2vec_lemma), axis=1)\n",
    "\n",
    "# Concaténation des embeddings FastText et Word2Vec pour stemming (600 Features)\n",
    "embedding_matrix_concat_stem = np.concatenate((embedding_matrix_fasttext_stem, embedding_matrix_word2vec_stem), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8e7d0c-b82b-4246-9625-c5a7952de600",
   "metadata": {},
   "source": [
    "# 4. Entraînement des Modèles LSTM et CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f9df4f-238c-4526-b643-761964c2935b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
