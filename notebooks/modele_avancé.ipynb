{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3864a408-216c-4169-b8d1-156187787123",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## Objectif du Notebook\n",
    "L'objectif de ce notebook est de développer et d'évaluer des modèles avancés d'analyse de sentiments en utilisant des architectures de réseaux de neurones convolutionnels (CNN) et de réseaux de neurones récurrents (LSTM). Ces modèles seront entraînés sur un jeu de données de tweets et utiliseront des embeddings textuels pré-entraînés tels que Word2Vec et FastText pour capturer les relations sémantiques entre les mots.\n",
    "\n",
    "## Présentation des Méthodes CNN et LSTM\n",
    "Le Convolutional Neural Network (CNN) et le Long Short-Term Memory (LSTM) sont deux types d'architectures de réseaux de neurones particulièrement adaptées au traitement des données séquentielles telles que le texte.\n",
    "\n",
    "### CNN (Convolutional Neural Network)\n",
    "Les CNN sont bien connus pour leur capacité à détecter des motifs locaux dans les données séquentielles, ce qui les rend efficaces pour capturer des n-grams de mots dans les textes. Dans ce notebook, nous utiliserons un CNN pour analyser les séquences de mots des tweets et en extraire des caractéristiques pertinentes pour la classification de sentiments.\n",
    "\n",
    "### LSTM (Long Short-Term Memory)\n",
    "Les LSTM sont un type de réseau de neurones récurrents (RNN) capable de capturer des dépendances à long terme dans des séquences de données. Cela les rend particulièrement efficaces pour comprendre le contexte complet d'une phrase ou d'un tweet, en tenant compte de l'ordre des mots et des relations temporelles.\n",
    "\n",
    "## Plan du Notebook\n",
    "Ce notebook est structuré de manière à suivre un pipeline complet de traitement des données, d'entraînement des modèles et d'évaluation des performances :\n",
    "1. **Chargement et Préparation des Données** : Nous allons commencer par charger les données, les nettoyer, les tokeniser et les préparer pour les modèles CNN et LSTM.\n",
    "2. **Préparation des Embeddings Textuels** : Nous utiliserons Word2Vec et FastText pour convertir les séquences de mots en vecteurs denses.\n",
    "3. **Construction du Modèle CNN** : Nous définirons et entraînerons un modèle CNN pour la tâche de classification de sentiments.\n",
    "4. **Construction du Modèle LSTM** : Nous définirons et entraînerons un modèle LSTM pour la même tâche.\n",
    "5. **Évaluation des Modèles** : Nous comparerons les performances des deux modèles pour déterminer lequel est le plus performant.\n",
    "6. **Exportation et Sauvegarde des Modèles** : Nous sauvegarderons le meilleur modèle pour une utilisation future.\n",
    "7. **Conclusion** : Nous résumerons les résultats et discuterons des perspectives d'amélioration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebd27e8-ae95-4de5-8a7b-bcf334c9a50c",
   "metadata": {},
   "source": [
    "# 1. Chargement et Préparation des Données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afabebb-2af2-4e07-9edb-0f4647a53152",
   "metadata": {},
   "source": [
    "## 1.1 Importation des bibliothèques nécessaires\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea7b0eea-7d9a-4f93-999e-77b2b351f479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import gensim.downloader as api\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665e4cf7-9c7b-44b5-b3ae-14d1665f63d2",
   "metadata": {},
   "source": [
    "## 1.2 Chargement des données et transformation de la cible target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5155889a-819c-4876-93fc-2b47bbb91dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in target_binary: [0 1]\n"
     ]
    }
   ],
   "source": [
    "# Charger les données\n",
    "data = pd.read_csv('../data/database_p7_rework.csv')\n",
    "\n",
    "# Transformation des labels : 0 reste 0 et 4 devient 1\n",
    "data['target_binary'] = data['target'].apply(lambda x: 0 if x == 0 else 1)\n",
    "\n",
    "# Vérification des transformations\n",
    "print(\"Unique values in target_binary:\", data['target_binary'].unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea28d151-0e30-495c-b588-c97f5422ee4e",
   "metadata": {},
   "source": [
    "## 1.3 Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44e64c99-36a7-46b6-8b47-aa24d9ad5464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour préparer les séquences\n",
    "def prepare_sequences(texts, tokenizer, maxlen=25):\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    return pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "# Tokenization\n",
    "tokenizer_lemma = Tokenizer(num_words=5000)\n",
    "tokenizer_lemma.fit_on_texts(data['text_lemmatized'])\n",
    "\n",
    "tokenizer_stem = Tokenizer(num_words=5000)\n",
    "tokenizer_stem.fit_on_texts(data['text_stemmed'])\n",
    "\n",
    "# Préparation des séquences\n",
    "X_lemma = prepare_sequences(data['text_lemmatized'], tokenizer_lemma)\n",
    "X_stem = prepare_sequences(data['text_stemmed'], tokenizer_stem)\n",
    "\n",
    "# Séparation des données\n",
    "y = data['target_binary']\n",
    "\n",
    "X_train_lemma, X_test_lemma, y_train_lemma, y_test_lemma = train_test_split(X_lemma, y, test_size=0.2, random_state=42)\n",
    "X_train_stem, X_test_stem, y_train_stem, y_test_stem = train_test_split(X_stem, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801c912b-abf8-4f13-9466-0dfcd15d1994",
   "metadata": {},
   "source": [
    "# 2 Préparation des Embeddings Textuels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc112644-a6dc-4b3b-b51d-4231fa62aeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les modèles Word2Vec et FastText pré-entraînés (300 dimensions)\n",
    "w2v_model = api.load('word2vec-google-news-300')\n",
    "ft_model = api.load('fasttext-wiki-news-subwords-300')\n",
    "\n",
    "# Fonction pour créer la matrice d'embeddings\n",
    "def create_embedding_matrix(tokenizer, embedding_model, embedding_dim=300):\n",
    "    embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        if word in embedding_model:\n",
    "            embedding_matrix[i] = embedding_model[word]\n",
    "    return embedding_matrix\n",
    "\n",
    "# Créer les matrices d'embeddings\n",
    "embedding_matrix_lemma_w2v = create_embedding_matrix(tokenizer_lemma, w2v_model)\n",
    "embedding_matrix_stem_w2v = create_embedding_matrix(tokenizer_stem, w2v_model)\n",
    "\n",
    "embedding_matrix_lemma_ft = create_embedding_matrix(tokenizer_lemma, ft_model)\n",
    "embedding_matrix_stem_ft = create_embedding_matrix(tokenizer_stem, ft_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561eef4e-2cce-4af0-88b1-13c5073f226a",
   "metadata": {},
   "source": [
    "# 3. Définition de l'architecture du modèle CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb98cbb8-ef58-4e65-b38c-25d4642be668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "\n",
    "# Fonction pour créer un modèle CNN\n",
    "def create_cnn_model(input_dim, embedding_matrix, max_length=100):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=embedding_matrix.shape[1], weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "    model.add(Conv1D(128, kernel_size=5, activation='relu'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a90500-c698-4075-ab0c-7398d74ff19e",
   "metadata": {},
   "source": [
    "# 4. Définition de l'architecture du modèle LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3627faf7-50fc-4a7e-84f4-8b427989061b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Bidirectional\n",
    "\n",
    "# Fonction pour créer un modèle LSTM\n",
    "def create_lstm_model(input_dim, embedding_matrix, max_length=100):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=embedding_matrix.shape[1], weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "    model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c832354b-711f-4a5d-8a62-5f36b8818b71",
   "metadata": {},
   "source": [
    "# 5. Entraînement et Évaluation des Modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "53cd41c9-ec4d-410b-a845-9fb362566937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import os\n",
    "\n",
    "# Définir le dossier racine où vous voulez enregistrer les artefacts MLflow\n",
    "base_dir = os.path.join(\"..\", \"mlruns\")\n",
    "\n",
    "# Liste des modèles que vous allez entraîner\n",
    "model_list = [\n",
    "    \"cnn_model_lemma_w2v\",\n",
    "    \"cnn_model_lemma_ft\",\n",
    "    \"cnn_model_stem_w2v\",\n",
    "    \"cnn_model_stem_ft\",\n",
    "    \"lstm_model_lemma_w2v\",\n",
    "    \"lstm_model_lemma_ft\",\n",
    "    \"lstm_model_stem_w2v\",\n",
    "    \"lstm_model_stem_ft\",\n",
    "]\n",
    "\n",
    "for model_name in model_list:\n",
    "    # Créer le chemin complet pour ce modèle spécifique\n",
    "    mlflow_base_dir = os.path.join(base_dir, model_name)\n",
    "\n",
    "    # Créer le dossier si nécessaire\n",
    "    if not os.path.exists(mlflow_base_dir):\n",
    "        os.makedirs(mlflow_base_dir)\n",
    "\n",
    "    # Configurer MLflow pour utiliser ce dossier comme URI de suivi\n",
    "    mlflow.set_tracking_uri(f\"file:///{mlflow_base_dir}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acb5d88-0a3f-47f1-a98d-5e3da2669827",
   "metadata": {},
   "source": [
    "## 5.1 Entraînement des Modèles CNN et LSTM avec les Embeddings Word2Vec + lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd4ab81-f57a-42a5-acdb-54d1f8910fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Master_Openclassroom\\python\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m15984/15984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 3ms/step - accuracy: 0.7744 - loss: 0.4704 - val_accuracy: 0.8025 - val_loss: 0.4255\n",
      "Epoch 2/10\n",
      "\u001b[1m15984/15984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 3ms/step - accuracy: 0.8089 - loss: 0.4148 - val_accuracy: 0.8067 - val_loss: 0.4189\n",
      "Epoch 3/10\n",
      "\u001b[1m15984/15984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 3ms/step - accuracy: 0.8193 - loss: 0.3959 - val_accuracy: 0.8078 - val_loss: 0.4166\n",
      "Epoch 4/10\n",
      "\u001b[1m15984/15984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 3ms/step - accuracy: 0.8272 - loss: 0.3814 - val_accuracy: 0.8091 - val_loss: 0.4158\n",
      "Epoch 5/10\n",
      "\u001b[1m15984/15984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 3ms/step - accuracy: 0.8322 - loss: 0.3702 - val_accuracy: 0.8085 - val_loss: 0.4202\n",
      "Epoch 6/10\n",
      "\u001b[1m15984/15984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 3ms/step - accuracy: 0.8386 - loss: 0.3591 - val_accuracy: 0.8045 - val_loss: 0.4254\n",
      "Epoch 7/10\n",
      "\u001b[1m15984/15984\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 3ms/step - accuracy: 0.8427 - loss: 0.3507 - val_accuracy: 0.8062 - val_loss: 0.4310\n",
      "Epoch 8/10\n",
      "\u001b[1m 7131/15984\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 3ms/step - accuracy: 0.8485 - loss: 0.3386"
     ]
    }
   ],
   "source": [
    "# Entraînement et évaluation pour chaque combinaison (lemmatisation/stemming et Word2Vec/FastText)\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "# CNN avec lemmatisation + Word2Vec\n",
    "with mlflow.start_run():\n",
    "    cnn_model_lemma_w2v = create_cnn_model(len(tokenizer_lemma.word_index) + 1, embedding_matrix_lemma_w2v)\n",
    "    cnn_model_lemma_w2v.fit(X_train_lemma, y_train_lemma, batch_size=64, epochs=10, validation_split=0.2, verbose=1)\n",
    "    mlflow.keras.log_model(cnn_model_lemma_w2v, \"cnn_model_lemma_w2v\")\n",
    "    y_pred_lemma_w2v = (cnn_model_lemma_w2v.predict(X_test_lemma) > 0.5).astype(\"int32\")\n",
    "    accuracy_lemma_w2v = accuracy_score(y_test_lemma, y_pred_lemma_w2v)\n",
    "    mlflow.log_metric(\"accuracy_lemma_w2v\", accuracy_lemma_w2v)\n",
    "    cm_lemma_w2v = confusion_matrix(y_test_lemma, y_pred_lemma_w2v)\n",
    "    plt.figure(figsize=(10,7))\n",
    "    sns.heatmap(cm_lemma_w2v, annot=True, fmt='d')\n",
    "    plt.title(\"Confusion Matrix: CNN + Lemmatization + Word2Vec\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.show()\n",
    "\n",
    "# LSTM avec lemmatisation + Word2Vec\n",
    "with mlflow.start_run():\n",
    "    lstm_model_lemma_w2v = create_lstm_model(len(tokenizer_lemma.word_index) + 1, embedding_matrix_lemma_w2v)\n",
    "    lstm_model_lemma_w2v.fit(X_train_lemma, y_train_lemma, batch_size=64, epochs=10, validation_split=0.2, verbose=1)\n",
    "    mlflow.keras.log_model(lstm_model_lemma_w2v, \"lstm_model_lemma_w2v\")\n",
    "    y_pred_lstm_lemma_w2v = (lstm_model_lemma_w2v.predict(X_test_lemma) > 0.5).astype(\"int32\")\n",
    "    accuracy_lstm_lemma_w2v = accuracy_score(y_test_lemma, y_pred_lstm_lemma_w2v)\n",
    "    mlflow.log_metric(\"accuracy_lstm_lemma_w2v\", accuracy_lstm_lemma_w2v)\n",
    "    cm_lstm_lemma_w2v = confusion_matrix(y_test_lemma, y_pred_lstm_lemma_w2v)\n",
    "    plt.figure(figsize=(10,7))\n",
    "    sns.heatmap(cm_lstm_lemma_w2v, annot=True, fmt='d')\n",
    "    plt.title(\"Confusion Matrix: LSTM + Lemmatization + Word2Vec\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304bb97c-dd32-45de-976e-d77da76972e9",
   "metadata": {},
   "source": [
    "## 5.2 Entraînement des Modèles CNN et LSTM avec les Embeddings FastText + lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9829ab4-1be6-4cfe-af96-c84cdbfe45eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN avec lemmatisation + FastText\n",
    "with mlflow.start_run():\n",
    "    cnn_model_lemma_ft = create_cnn_model(len(tokenizer_lemma.word_index) + 1, embedding_matrix_lemma_ft)\n",
    "    cnn_model_lemma_ft.fit(X_train_lemma, y_train_lemma, batch_size=64, epochs=10, validation_split=0.2, verbose=1)\n",
    "    mlflow.keras.log_model(cnn_model_lemma_ft, \"cnn_model_lemma_ft\")\n",
    "    y_pred_lemma_ft = (cnn_model_lemma_ft.predict(X_test_lemma) > 0.5).astype(\"int32\")\n",
    "    accuracy_lemma_ft = accuracy_score(y_test_lemma, y_pred_lemma_ft)\n",
    "    mlflow.log_metric(\"accuracy_lemma_ft\", accuracy_lemma_ft)\n",
    "    cm_lemma_ft = confusion_matrix(y_test_lemma, y_pred_lemma_ft)\n",
    "    plt.figure(figsize=(10,7))\n",
    "    sns.heatmap(cm_lemma_ft, annot=True, fmt='d')\n",
    "    plt.title(\"Confusion Matrix: CNN + Lemmatization + FastText\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.show()\n",
    "\n",
    "# LSTM avec lemmatisation + FastText\n",
    "with mlflow.start_run():\n",
    "    lstm_model_lemma_ft = create_lstm_model(len(tokenizer_lemma.word_index) + 1, embedding_matrix_lemma_ft)\n",
    "    lstm_model_lemma_ft.fit(X_train_lemma, y_train_lemma, batch_size=64, epochs=10, validation_split=0.2, verbose=1)\n",
    "    mlflow.keras.log_model(lstm_model_lemma_ft, \"lstm_model_lemma_ft\")\n",
    "    y_pred_lstm_lemma_ft = (lstm_model_lemma_ft.predict(X_test_lemma) > 0.5).astype(\"int32\")\n",
    "    accuracy_lstm_lemma_ft = accuracy_score(y_test_lemma, y_pred_lstm_lemma_ft)\n",
    "    mlflow.log_metric(\"accuracy_lstm_lemma_ft\", accuracy_lstm_lemma_ft)\n",
    "    cm_lstm_lemma_ft = confusion_matrix(y_test_lemma, y_pred_lstm_lemma_ft)\n",
    "    plt.figure(figsize=(10,7))\n",
    "    sns.heatmap(cm_lstm_lemma_ft, annot=True, fmt='d')\n",
    "    plt.title(\"Confusion Matrix: LSTM + Lemmatization + FastText\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576bd1d2-089d-4124-9b5a-441b07174ce5",
   "metadata": {},
   "source": [
    "## 5.3 Entraînement des Modèles CNN et LSTM avec les Embeddings Word2Vec + stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afd59a7-76a3-46ce-a70f-31dc02199560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN avec stemming + Word2Vec\n",
    "with mlflow.start_run():\n",
    "    cnn_model_stem_w2v = create_cnn_model(len(tokenizer_stem.word_index) + 1, embedding_matrix_stem_w2v)\n",
    "    cnn_model_stem_w2v.fit(X_train_stem, y_train_stem, batch_size=64, epochs=10, validation_split=0.2, verbose=1)\n",
    "    mlflow.keras.log_model(cnn_model_stem_w2v, \"cnn_model_stem_w2v\")\n",
    "    y_pred_stem_w2v = (cnn_model_stem_w2v.predict(X_test_stem) > 0.5).astype(\"int32\")\n",
    "    accuracy_stem_w2v = accuracy_score(y_test_stem, y_pred_stem_w2v)\n",
    "    mlflow.log_metric(\"accuracy_stem_w2v\", accuracy_stem_w2v)\n",
    "    cm_stem_w2v = confusion_matrix(y_test_stem, y_pred_stem_w2v)\n",
    "    plt.figure(figsize=(10,7))\n",
    "    sns.heatmap(cm_stem_w2v, annot=True, fmt='d')\n",
    "    plt.title(\"Confusion Matrix: CNN + Stemming + Word2Vec\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.show()\n",
    "\n",
    "# LSTM avec stemming + Word2Vec\n",
    "with mlflow.start_run():\n",
    "    lstm_model_stem_w2v = create_lstm_model(len(tokenizer_stem.word_index) + 1, embedding_matrix_stem_w2v)\n",
    "    lstm_model_stem_w2v.fit(X_train_stem, y_train_stem, batch_size=64, epochs=10, validation_split=0.2, verbose=1)\n",
    "    mlflow.keras.log_model(lstm_model_stem_w2v, \"lstm_model_stem_w2v\")\n",
    "    y_pred_lstm_stem_w2v = (lstm_model_stem_w2v.predict(X_test_stem) > 0.5).astype(\"int32\")\n",
    "    accuracy_lstm_stem_w2v = accuracy_score(y_test_stem, y_pred_lstm_stem_w2v)\n",
    "    mlflow.log_metric(\"accuracy_lstm_stem_w2v\", accuracy_lstm_stem_w2v)\n",
    "    cm_lstm_stem_w2v = confusion_matrix(y_test_stem, y_pred_lstm_stem_w2v)\n",
    "    plt.figure(figsize=(10,7))\n",
    "    sns.heatmap(cm_lstm_stem_w2v, annot=True, fmt='d')\n",
    "    plt.title(\"Confusion Matrix: LSTM + Stemming + Word2Vec\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18923088-9975-46d1-a9df-24aaad456a64",
   "metadata": {},
   "source": [
    "## 5.4 Entraînement des Modèles CNN et LSTM avec les Embeddings FastText + stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44956185-bc70-43a8-939e-f93e2d128959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN avec stemming + FastText\n",
    "with mlflow.start_run():\n",
    "    cnn_model_stem_ft = create_cnn_model(len(tokenizer_stem.word_index) + 1, embedding_matrix_stem_ft)\n",
    "    cnn_model_stem_ft.fit(X_train_stem, y_train_stem, batch_size=64, epochs=10, validation_split=0.2, verbose=1)\n",
    "    mlflow.keras.log_model(cnn_model_stem_ft, \"cnn_model_stem_ft\")\n",
    "    y_pred_stem_ft = (cnn_model_stem_ft.predict(X_test_stem) > 0.5).astype(\"int32\")\n",
    "    accuracy_stem_ft = accuracy_score(y_test_stem, y_pred_stem_ft)\n",
    "    mlflow.log_metric(\"accuracy_stem_ft\", accuracy_stem_ft)\n",
    "    cm_stem_ft = confusion_matrix(y_test_stem, y_pred_stem_ft)\n",
    "    plt.figure(figsize=(10,7))\n",
    "    sns.heatmap(cm_stem_ft, annot=True, fmt='d')\n",
    "    plt.title(\"Confusion Matrix: CNN + Stemming + FastText\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.show()\n",
    "\n",
    "# LSTM avec stemming + FastText\n",
    "with mlflow.start_run():\n",
    "    lstm_model_stem_ft = create_lstm_model(len(tokenizer_stem.word_index) + 1, embedding_matrix_stem_ft)\n",
    "    lstm_model_stem_ft.fit(X_train_stem, y_train_stem, batch_size=64, epochs=10, validation_split=0.2, verbose=1)\n",
    "    mlflow.keras.log_model(lstm_model_stem_ft, \"lstm_model_stem_ft\")\n",
    "    y_pred_lstm_stem_ft = (lstm_model_stem_ft.predict(X_test_stem) > 0.5).astype(\"int32\")\n",
    "    accuracy_lstm_stem_ft = accuracy_score(y_test_stem, y_pred_lstm_stem_ft)\n",
    "    mlflow.log_metric(\"accuracy_lstm_stem_ft\", accuracy_lstm_stem_ft)\n",
    "    cm_lstm_stem_ft = confusion_matrix(y_test_stem, y_pred_lstm_stem_ft)\n",
    "    plt.figure(figsize=(10,7))\n",
    "    sns.heatmap(cm_lstm_stem_ft, annot=True, fmt='d')\n",
    "    plt.title(\"Confusion Matrix: LSTM + Stemming + FastText\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff96ef25-bc5c-4eed-8c9b-637859c8eada",
   "metadata": {},
   "source": [
    "# 6. Comparaison des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c075b7-fdc6-4368-98fa-33ae9a6941f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialiser une liste pour stocker les résultats\n",
    "results = []\n",
    "\n",
    "# Fonction pour ajouter les résultats d'un modèle à la liste\n",
    "def log_results(model_name, accuracy, precision, recall, f1):\n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1\n",
    "    })\n",
    "\n",
    "# Log des résultats pour les modèles avancés (Word2Vec + FastText)\n",
    "\n",
    "# CNN + Lemmatization + Word2Vec\n",
    "log_results(\n",
    "    'CNN + Lemmatization + Word2Vec',\n",
    "    accuracy_cnn_lemma_w2v,\n",
    "    precision_cnn_lemma_w2v,\n",
    "    recall_cnn_lemma_w2v,\n",
    "    f1_cnn_lemma_w2v\n",
    ")\n",
    "\n",
    "# LSTM + Lemmatization + Word2Vec\n",
    "log_results(\n",
    "    'LSTM + Lemmatization + Word2Vec',\n",
    "    accuracy_lstm_lemma_w2v,\n",
    "    precision_lstm_lemma_w2v,\n",
    "    recall_lstm_lemma_w2v,\n",
    "    f1_lstm_lemma_w2v\n",
    ")\n",
    "\n",
    "# CNN + Stemming + Word2Vec\n",
    "log_results(\n",
    "    'CNN + Stemming + Word2Vec',\n",
    "    accuracy_cnn_stem_w2v,\n",
    "    precision_cnn_stem_w2v,\n",
    "    recall_cnn_stem_w2v,\n",
    "    f1_cnn_stem_w2v\n",
    ")\n",
    "\n",
    "# LSTM + Stemming + Word2Vec\n",
    "log_results(\n",
    "    'LSTM + Stemming + Word2Vec',\n",
    "    accuracy_lstm_stem_w2v,\n",
    "    precision_lstm_stem_w2v,\n",
    "    recall_lstm_stem_w2v,\n",
    "    f1_lstm_stem_w2v\n",
    ")\n",
    "\n",
    "# CNN + Lemmatization + FastText\n",
    "log_results(\n",
    "    'CNN + Lemmatization + FastText',\n",
    "    accuracy_cnn_lemma_ft,\n",
    "    precision_cnn_lemma_ft,\n",
    "    recall_cnn_lemma_ft,\n",
    "    f1_cnn_lemma_ft\n",
    ")\n",
    "\n",
    "# LSTM + Lemmatization + FastText\n",
    "log_results(\n",
    "    'LSTM + Lemmatization + FastText',\n",
    "    accuracy_lstm_lemma_ft,\n",
    "    precision_lstm_lemma_ft,\n",
    "    recall_lstm_lemma_ft,\n",
    "    f1_lstm_lemma_ft\n",
    ")\n",
    "\n",
    "# CNN + Stemming + FastText\n",
    "log_results(\n",
    "    'CNN + Stemming + FastText',\n",
    "    accuracy_cnn_stem_ft,\n",
    "    precision_cnn_stem_ft,\n",
    "    recall_cnn_stem_ft,\n",
    "    f1_cnn_stem_ft\n",
    ")\n",
    "\n",
    "# LSTM + Stemming + FastText\n",
    "log_results(\n",
    "    'LSTM + Stemming + FastText',\n",
    "    accuracy_lstm_stem_ft,\n",
    "    precision_lstm_stem_ft,\n",
    "    recall_lstm_stem_ft,\n",
    "    f1_lstm_stem_ft\n",
    ")\n",
    "\n",
    "# Convertir les résultats en DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Afficher le tableau récapitulatif\n",
    "print(results_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea52d0c5-e0dd-4633-b42d-b111064e7c12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
