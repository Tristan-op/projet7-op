{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eee547d-d095-4364-b0ad-543b85bf7f67",
   "metadata": {},
   "source": [
    "# Introduction\r\n",
    "\r\n",
    "## Objectif du Notebook\r\n",
    "\r\n",
    "L'objectif de ce notebook est de développer et d'évaluer des modèles d'analyse de sentiments en utilisant **BERT** (Bidirectional Encoder Representations from Transformers), tout en comparant différentes versions des données textuelles : avec et sans pré-traitements (lemmatisation et stemming). BERT, étant un modèle pré-entraîné puissant, nous permet de capturer les relations contextuelles entre les mots dans un texte. En parallèle, nous allons comparer les performances des modèles CNN et LSTM entraînés sur des textes bruts et pré-traités.\r\n",
    "\r\n",
    "## Présentation de la Méthode BERT\r\n",
    "\r\n",
    "BERT est un modèle de traitement du langage naturel basé sur une architecture de **Transformers**, permettant de capturer de manière bidirectionnelle les relations entre les mots. BERT est particulièrement efficace pour des tâches telles que l'analyse de sentiments car il tient compte du contexte des mots dans les phrases, contrairement aux modèles classiques comme Word2Vec qui sont unidirectionnels.\r\n",
    "\r\n",
    "### BERT (Bidirectional Encoder Representations from Transformers)\r\n",
    "\r\n",
    "BERT prend en compte le contexte avant et après un mot pour créer des représentations contextuelles riches. C'est pourquoi il est très performant pour des tâches de compréhension sémantique, comme la classification de sentiments, où l'interprétation d'un mot dépend de son contexte global.\r\n",
    "\r\n",
    "### Comparaison avec et sans Pré-traitements (Lemmatisation et Stemming)\r\n",
    "\r\n",
    "Dans ce notebook, nous utiliserons à la fois :\r\n",
    "- **Les textes pré-traités** (lemmatisation et stemming) : Ces techniques consistent à ramener les mots à leur forme de base ou à leur racine pour réduire la variabilité linguistique. Cela permet de simplifier les phrases avant de les soumettre à BERT.\r\n",
    "- **Les textes bruts** : Sans aucun pré-traitement linguistique, laissant à BERT la capacité d'analyser les mots dans leur forme originale et de capturer les nuances du langage.\r\n",
    "\r\n",
    "L'objectif est de comparer l'impact du pré-traitement sur les performances du modèle BERT.\r\n",
    "\r\n",
    "## Plan du Notebook\r\n",
    "\r\n",
    "1. **Chargement et Préparation des Données** : Nous allons charger les données, les nettoyer, les tokeniser, et les préparer sous trois formats : brut, lemmatisé et stemmé.\r\n",
    "2. **Tokenisation avec BERT** : Nous utiliserons le tokenizer BERT pour convertir les trois versions de textes en tokens adaptés au modèle BERT.\r\n",
    "3. **Utilisation de BERT** : Nous extrairons des embeddings contextuels à partir du modèle BERT pour chaque version des données.\r\n",
    "4. **Construction des Modèles CNN et LSTM** : Nous entraînerons deux types de modèles (CNN et LSTM) sur les embeddings extraits de BERT pour chaque version de données.\r\n",
    "5. **Entraînement des Modèles** : Nous entraînerons les modèles sur les différentes versions des embeddings BERT.\r\n",
    "6. **Comparaison des Modèles** : Nous comparerons les performances des modèles CNN et LSTM avec les embeddings BERT pour les données brutes, lemmatisées et stemmées.\r\n",
    "7. **Export du Meilleur Modèle** : Nous exporterons le modèle le plus performant pour une utilisation future dans une API de prédiction.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b06c2d-a906-4bc0-9671-d17f37ca444e",
   "metadata": {},
   "source": [
    "# 1. Importation des Bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b4d6b57f-afaf-42b2-8a82-95ec5280fdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import BertTokenizer, TFBertModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7f43e4-1b2a-44dd-9f34-cdcc0e25c028",
   "metadata": {},
   "source": [
    "# 2. Chargement des Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "866990a8-50a0-4856-9c67-ce0f2796da95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    800000\n",
      "1    798315\n",
      "Name: target_binary, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Charger les données (en supposant que vous avez déjà les colonnes nettoyées)\n",
    "data = pd.read_csv('../data/database_p7_rework.csv')\n",
    "\n",
    "# Transformation des labels : 0 reste 0 et 4 devient 1\n",
    "data['target_binary'] = data['target'].apply(lambda x: 0 if x == 0 else 1)\n",
    "\n",
    "# Vérification des transformations\n",
    "print(data['target_binary'].value_counts())\n",
    "\n",
    "# Ensuite, définissez y comme suit :\n",
    "y = data['target_binary']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66521c0d-02bd-4dd7-9e21-b4ccf09838ab",
   "metadata": {},
   "source": [
    "# 3. Préparation des Données\n",
    "Tokenization avec BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4714e1f5-06a4-4f3c-9acc-0bfed02f8185",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Master_Openclassroom\\python\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données préparées avec succès.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialisation du tokenizer BERT pré-entraîné pour les textes en minuscules et sans caractères spéciaux\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Fonction pour préparer les séquences BERT\n",
    "def prepare_sequences_bert(texts, tokenizer, maxlen=100):\n",
    "    return tokenizer(texts.tolist(), padding='max_length', truncation=True, max_length=maxlen, return_tensors='tf')\n",
    "\n",
    "# Sélection d'un échantillon équilibré de 16 000 données (8 000 par classe)\n",
    "sample_data = data.groupby('target_binary', group_keys=False).apply(lambda x: x.sample(8000, random_state=42))\n",
    "\n",
    "# Préparation des séquences avec une longueur max de 100\n",
    "X_cleaned_sample = prepare_sequences_bert(sample_data['text_cleaned'], tokenizer, maxlen=100)\n",
    "\n",
    "# Cible (target) - identique pour toutes les variantes\n",
    "y_sample = sample_data['target_binary'].values\n",
    "\n",
    "print(\"Données préparées avec succès.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7cb38c73-5e7f-40b8-9ebf-9db5455a631d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle BERT chargé avec succès.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertModel\n",
    "\n",
    "# Charger le modèle BERT pré-entraîné\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "print(\"Modèle BERT chargé avec succès.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "af2eb07b-683a-4270-ac83-dd1a2b525a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Générateur de batchs pour BERT\n",
    "def bert_embedding_generator(sequences, labels, attention_masks, batch_size=32):\n",
    "    while True:\n",
    "        for i in range(0, len(sequences), batch_size):\n",
    "            batch_sequences = sequences[i:i + batch_size]\n",
    "            batch_labels = labels[i:i + batch_size]\n",
    "            batch_attention_masks = attention_masks[i:i + batch_size]\n",
    "            yield {\n",
    "                'input_ids': batch_sequences, \n",
    "                'attention_mask': batch_attention_masks\n",
    "            }, batch_labels  # Retourner les séquences, masques d'attention et labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "00436f0c-009f-49ba-9e47-08beb5eaf829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Fonction pour obtenir les embeddings BERT par batchs\n",
    "def get_bert_embeddings_in_batches(generator, bert_model):\n",
    "    embeddings = []\n",
    "    for batch_data, batch_labels in generator:\n",
    "        batch_input_ids = batch_data['input_ids']\n",
    "        batch_attention_mask = batch_data['attention_mask']\n",
    "        \n",
    "        # Générer les embeddings pour chaque batch\n",
    "        batch_embeddings = bert_model(input_ids=batch_input_ids, attention_mask=batch_attention_mask).last_hidden_state\n",
    "        embeddings.append(batch_embeddings)\n",
    "\n",
    "    return tf.concat(embeddings, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "06ead7bd-d577-4c05-ad2a-5b7f9ab3c009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données scindées en ensemble d'entraînement et de test.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Récupérer les input_ids et attention_mask des séquences préparées\n",
    "X_cleaned_sample_input_ids = X_cleaned_sample['input_ids'].numpy()  # Convertir en numpy\n",
    "X_cleaned_sample_attention_mask = X_cleaned_sample['attention_mask'].numpy()  # Convertir en numpy\n",
    "\n",
    "# Split en données d'entraînement et de test avec stratification\n",
    "X_train_cleaned_ids, X_test_cleaned_ids, y_train_cleaned, y_test_cleaned = train_test_split(\n",
    "    X_cleaned_sample_input_ids, y_sample, test_size=0.2, random_state=42, stratify=y_sample)\n",
    "\n",
    "X_train_mask, X_test_mask = train_test_split(\n",
    "    X_cleaned_sample_attention_mask, test_size=0.2, random_state=42, stratify=y_sample)\n",
    "\n",
    "print(\"Données scindées en ensemble d'entraînement et de test.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36fb1d0-0283-43a9-9ab9-066869a18d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer les générateurs\n",
    "train_generator = bert_embedding_generator(X_train_cleaned_ids, y_train_cleaned, X_train_mask, batch_size=32)\n",
    "test_generator = bert_embedding_generator(X_test_cleaned_ids, y_test_cleaned, X_test_mask, batch_size=32)\n",
    "\n",
    "# Générer les embeddings pour les ensembles d'entraînement et de test\n",
    "X_train_embeddings = get_bert_embeddings_in_batches(train_generator, bert_model)\n",
    "X_test_embeddings = get_bert_embeddings_in_batches(test_generator, bert_model)\n",
    "\n",
    "print(\"Embeddings BERT générés pour les ensembles d'entraînement et de test.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fb25f8-8675-4087-885d-8719cfa87b80",
   "metadata": {},
   "source": [
    "# Construction des modèles CNN et LSTM avec BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b77c4b-dad4-4ad5-803e-6f047b03ef65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.keras\n",
    "import time\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, Dense, Dropout, BatchNormalization, LeakyReLU, PReLU\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "def create_cnn_model(max_length=100, num_filters=128, kernel_size=5, dropout_rate=0.2, activation_type='relu'):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Couche de convolution et normalisation\n",
    "    model.add(Conv1D(num_filters, kernel_size=kernel_size, activation='relu', input_shape=(max_length, 300)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    \n",
    "    # Ajout de l'activation dynamique dans les couches denses\n",
    "    if activation_type == 'relu':\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "    elif activation_type == 'leaky_relu':\n",
    "        model.add(Dense(128))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "    elif activation_type == 'prelu':\n",
    "        model.add(Dense(128))\n",
    "        model.add(PReLU())\n",
    "\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# Fonction pour entraîner et loguer un modèle CNN avec GridSearch et les activations avancées\n",
    "def train_and_log_cnn(X_train, y_train, X_test, y_test, experiment_name, param_grid, max_length=100):\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    \n",
    "    best_model = None\n",
    "    best_accuracy = 0\n",
    "    best_params = None\n",
    "\n",
    "    # Parcourir chaque combinaison d'hyperparamètres\n",
    "    for params in ParameterGrid(param_grid):\n",
    "        num_filters = params['num_filters']\n",
    "        kernel_size = params['kernel_size']\n",
    "        dropout_rate = params['dropout_rate']\n",
    "        activation_type = params['activation_type']\n",
    "\n",
    "        with mlflow.start_run(run_name=f\"CNN_filters={num_filters}_kernel={kernel_size}_dropout={dropout_rate}_activation={activation_type}\"):\n",
    "\n",
    "            # Créer le modèle CNN avec les hyperparamètres courants\n",
    "            model = create_cnn_model(max_length=max_length, num_filters=num_filters, kernel_size=kernel_size, dropout_rate=dropout_rate, activation_type=activation_type)\n",
    "\n",
    "            # Early stopping pour éviter l'overfitting\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=4)\n",
    "            reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001)\n",
    "\n",
    "            # Entraîner le modèle\n",
    "            start_time = time.time()\n",
    "            history = model.fit(X_train, y_train, epochs=20, batch_size=64, validation_data=(X_test, y_test), callbacks=[early_stopping, reduce_lr], verbose=1)\n",
    "            training_time = time.time() - start_time\n",
    "\n",
    "            # Prédictions et évaluation\n",
    "            y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "            y_pred_proba = model.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "            precision = precision_score(y_test, y_pred)\n",
    "            recall = recall_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "            # Matrice de confusion\n",
    "            conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "            # Loguer les hyperparamètres dans MLFlow\n",
    "            mlflow.log_param(\"num_filters\", num_filters)\n",
    "            mlflow.log_param(\"kernel_size\", kernel_size)\n",
    "            mlflow.log_param(\"dropout_rate\", dropout_rate)\n",
    "            mlflow.log_param(\"activation_type\", activation_type)\n",
    "\n",
    "            # Loguer les métriques dans MLFlow\n",
    "            mlflow.log_metric(\"accuracy\", accuracy)\n",
    "            mlflow.log_metric(\"auc\", auc_score)\n",
    "            mlflow.log_metric(\"precision\", precision)\n",
    "            mlflow.log_metric(\"recall\", recall)\n",
    "            mlflow.log_metric(\"f1_score\", f1)\n",
    "            mlflow.log_metric(\"training_time\", training_time)\n",
    "\n",
    "            # Sauvegarder le modèle avec MLFlow\n",
    "            mlflow.keras.log_model(model, f\"cnn_model_{num_filters}_{kernel_size}_{dropout_rate}\")\n",
    "\n",
    "            # Sauvegarder et loguer la matrice de confusion\n",
    "            plt.figure(figsize=(6, 4))\n",
    "            sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "            plt.xlabel('Prédictions')\n",
    "            plt.ylabel('Vérités')\n",
    "            plt.title(f\"Matrice de Confusion - CNN\")\n",
    "            conf_matrix_path = f\"./matrice/confusion_matrix_cnn_filters={num_filters}_kernel={kernel_size}_dropout={dropout_rate}.png\"\n",
    "            plt.savefig(conf_matrix_path)\n",
    "            mlflow.log_artifact(conf_matrix_path)\n",
    "            plt.close()  # Fermer la figure pour éviter l'affichage dans le notebook\n",
    "\n",
    "            # Sauvegarder et loguer la courbe ROC\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "            plt.figure()\n",
    "            plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {auc_score:.2f})\")\n",
    "            plt.xlabel(\"False Positive Rate\")\n",
    "            plt.ylabel(\"True Positive Rate\")\n",
    "            plt.title(\"ROC Curve\")\n",
    "            plt.legend(loc=\"best\")\n",
    "            roc_curve_path = f\"./matrice/roc_curve_cnn_filters={num_filters}_kernel={kernel_size}_dropout={dropout_rate}.png\"\n",
    "            plt.savefig(roc_curve_path)\n",
    "            mlflow.log_artifact(roc_curve_path)\n",
    "            plt.close()  # Fermer la figure pour éviter l'affichage dans le notebook\n",
    "\n",
    "            # Comparer pour garder le meilleur modèle\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_params = params\n",
    "                best_model = model\n",
    "\n",
    "    print(f\"Meilleurs paramètres : {best_params} avec une accuracy de {best_accuracy:.4f}\")\n",
    "\n",
    "    # Retourner le meilleur modèle\n",
    "    return best_model, best_params\n",
    "\n",
    "\n",
    "\n",
    "# GridSearch pour le modèle CNN avec les hyperparamètres supplémentaires\n",
    "param_grid_cnn = {\n",
    "    'num_filters': [64, 128, 256],   # Filtres à tester\n",
    "    'kernel_size': [3, 5],        # Tailles de kernel à tester\n",
    "    'dropout_rate': [0.2, 0.5],      # Dropout à tester\n",
    "    'activation_type': ['relu', 'leaky_relu', 'prelu']  # Activations à tester\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c004a2fd-2936-487e-9002-31ed2532b031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraîner le modèle avec les données correctement formatées\n",
    "best_cnn_model_bert, best_params_bert = train_and_log_cnn(\n",
    "    X_train_bert, y_train_bert, X_test_bert, y_test_bert, \n",
    "    experiment_name=\"CNN_bert\", param_grid=param_grid_cnn, max_length=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "75d6ce06-4d55-4f8f-adb9-51deccc27675",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, Bidirectional, BatchNormalization, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Fonction pour créer un modèle LSTM sans la couche d'embedding, car les embeddings BERT sont déjà calculés\n",
    "def create_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(256, dropout=0.3, recurrent_dropout=0.3, input_shape=input_shape)))  # LSTM bidirectionnel\n",
    "    model.add(BatchNormalization())  # Ajout de BatchNorm\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.4))  # Augmenter Dropout\n",
    "    model.add(Dense(64, activation='relu'))  # Couche Dense supplémentaire\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9913725-55b9-4ec4-a950-aa1bd2557f7f",
   "metadata": {},
   "source": [
    "# 4 Evaluation des modèles avec MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "72c802d8-ef5e-4422-a8bc-ac99abfb0b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir le dossier racine où vous voulez enregistrer les artefacts MLflow\n",
    "base_dir = os.path.join(\"..\", \"mlruns\")\n",
    "\n",
    "# Fonction pour configurer MLflow pour chaque modèle\n",
    "def configure_mlflow(model_name):\n",
    "    mlflow_base_dir = os.path.join(base_dir, model_name)\n",
    "    if not os.path.exists(mlflow_base_dir):\n",
    "        os.makedirs(mlflow_base_dir)\n",
    "    mlflow.set_tracking_uri(f\"file:///{mlflow_base_dir.replace(os.sep, '/')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1870b5f4-b527-4012-80fc-4c1397c38013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd1a99c-01a6-43cd-a07a-b537c636aef2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
